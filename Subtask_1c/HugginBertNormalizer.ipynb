{"cells":[{"cell_type":"markdown","metadata":{"id":"nESijxJq9v3T"},"source":["### Mount Google drive\n","\n","*  Mount Google drive in the directory '/content/drive'\n","*  Drive contains dataset files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17385,"status":"ok","timestamp":1666045317988,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"x02lCdasDfiM","outputId":"59905a55-6f71-4328-dcc8-7999ef343590"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"KsquP1OCDfiP"},"source":["### Install packages\n","\n","*  `tf-models-official` is the stable Model Garden package. Note that it may not include the latest changes in the `tensorflow_models` github repo. Not needed in currently when running script in Kaggle\n","*  `transformers` package\n","*  `datasets` package\n","*  pip will install all models and dependencies automatically."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19720,"status":"ok","timestamp":1666045338566,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"lMFQFgnZDfiS","outputId":"32b051ad-8748-4f45-8322-d07b3eae45d9"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14293,"status":"ok","timestamp":1666045352852,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"7mOxflv2DfiT","outputId":"6f700692-0492-433c-dcfa-90bf145268f5"},"outputs":[],"source":["!pip install datasets"]},{"cell_type":"markdown","metadata":{"id":"XCWJT3TSDfiU"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13120,"status":"ok","timestamp":1666045365967,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"OV1kEYatDfiV"},"outputs":[],"source":["import os\n","from collections import Counter\n","\n","import csv\n","import pandas as pd\n","\n","from transformers import DataCollatorWithPadding\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import create_optimizer\n","from transformers.keras_callbacks import KerasMetricCallback\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","import tensorflow as tf\n","\n","import datasets\n","from datasets import Dataset\n","from datasets import ClassLabel, Value\n","\n","import numpy as np\n","\n","from sklearn.model_selection import StratifiedShuffleSplit"]},{"cell_type":"markdown","metadata":{"id":"ZoulOuqLDfiX"},"source":["### Preprocessing dataset\n","\n","- Read Augmented data dataset\n","- Change label types"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1212,"status":"ok","timestamp":1666045367174,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"5yiTl8t8DfiZ","outputId":"ee4206c1-4917-412e-e83a-e8a60830dafb"},"outputs":[],"source":["# Path to datasets\n","\n","training_set = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/training_data/merged_training_dataset.tsv\"\n","validation_set = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/validation_data/merged_validation_dataset.tsv\"\n","\n","df = pd.read_csv(training_set, sep='\\t', quoting=csv.QUOTE_NONE)\n","validation_df = pd.read_csv(validation_set, sep='\\t', quoting=csv.QUOTE_NONE)\n","\n","df = df.astype({\"label\": int})\n","validation_df = validation_df.astype({\"label\": int})\n","\n","med_id_dict = {}\n","\n","train_id_set = set(Counter(df[\"label\"]).keys())\n","map_id = 0\n","for id in train_id_set:\n","  df.loc[ df[\"label\"] == id, \"label\"] = map_id\n","  med_id_dict[id] = map_id\n","  map_id = map_id + 1\n","\n","print(\"Training set has\", len(train_id_set), \"unique MEDDRA IDs.\")\n","\n","val_id_set = set(Counter(validation_df[\"label\"]).keys())\n","for id in val_id_set:\n","  if id in med_id_dict.keys():\n","    validation_df.loc[ validation_df[\"label\"] == id, \"label\"] = med_id_dict[id]\n","  else:\n","    validation_df.loc[ validation_df[\"label\"] == id, \"label\"] = map_id\n","    med_id_dict[id] = map_id\n","    map_id = map_id + 1\n","\n","print(\"Validation set has\", len(val_id_set), \"unique MEDDRA IDs.\")\n","\n","dataset = Dataset.from_pandas(df)\n","validation_dataset = Dataset.from_pandas(validation_df)\n","\n","num_labels = map_id\n","\n","print(\"Both sets have\", (num_labels), \"unique MEDDRA IDs.\")\n","\n","print(dataset.features)\n","print(validation_dataset.features)"]},{"cell_type":"markdown","metadata":{"id":"3Thcye_sQ8Kr"},"source":["### WEBRADR Reference Dataset Pre-processing\n","\n","- Processing MedDRA dataset\n","- Matching WEBRADR mentions to the MedDRA IDs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1963,"status":"ok","timestamp":1666045369132,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"0bv3rPuRQ8hQ"},"outputs":[],"source":["meddra_path = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/meddra/meddra.tsv\"\n","\n","meddra_df = pd.read_csv(meddra_path, sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n","\n","meddra_dict = {}\n","\n","for i in range(len(meddra_df)):\n","  if meddra_df[1][i] == \"PT\":\n","    term = meddra_df[3][i].lower()\n","    term_id = meddra_df[2][i]\n","    if term in meddra_dict.keys():\n","      print(\"Repeated term appeared\")\n","    meddra_dict[term] = term_id\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1666045456275,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"4RmVhZulXnr0","outputId":"659f6ec8-babc-48f7-fea3-b3ccbe4132da"},"outputs":[],"source":["reference_set = \"/content/drive/MyDrive/Dissertacao/IMI_WEBRADR_Reference_Dataset/T2_MOESM_dataset.tsv\"\n","\n","reference_df = pd.read_csv(reference_set, sep='\\t', quoting=csv.QUOTE_NONE)\n","\n","not_found = 0\n","not_found_set = set()\n","\n","reference_dataset_dict = {\"tweet_id\":[], \"text\":[], \"label\":[], \"span\":[]}\n","\n","\n","for i in range(len(reference_df)):\n","  if reference_df[\"preferred_term\"][i].lower() not in meddra_dict.keys():\n","    #print(\"Preferred term not in meddra dictionary!!!\")\n","    #print(\"Term not found:\", reference_df[\"preferred_term\"][i].lower())\n","    not_found = not_found + 1\n","    not_found_set.add(reference_df[\"preferred_term\"][i].lower())\n","    continue\n","  reference_dataset_dict[\"tweet_id\"].append(reference_df[\"tweet_id\"][i])\n","  reference_dataset_dict[\"text\"].append(reference_df[\"text\"][i])\n","  reference_dataset_dict[\"label\"].append(meddra_dict[reference_df[\"preferred_term\"][i].lower()])\n","  reference_dataset_dict[\"span\"].append(reference_df[\"span\"][i])\n","  \n","\n","print(\"Found:\", len(reference_df) - not_found)\n","print(\"Not found:\", not_found)\n","print(\"Unique not found:\", len(not_found_set))\n"]},{"cell_type":"markdown","metadata":{"id":"9dxPX0wCiG7l"},"source":["### (Pre-requisite to) Adding other datasets to training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8Evvsv8ijWU"},"outputs":[],"source":["dataset_dict = {\"text\":[], \"label\":[], \"span\":[]}\n","\n","for i in range(len(df)):\n","  dataset_dict[\"text\"].append(df[\"text\"][i])\n","  dataset_dict[\"label\"].append(df[\"label\"][i])\n","  dataset_dict[\"span\"].append(df[\"span\"][i])\n"]},{"cell_type":"markdown","metadata":{"id":"g5Vjtm4dHkem"},"source":["#### Adding validation dataset to training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxdn03MYHryN"},"outputs":[],"source":["val_dataset_dict = {\"text\":[], \"label\":[], \"span\":[]}\n","\n","for i in range(len(validation_df)):\n","  val_dataset_dict[\"text\"].append(validation_df[\"text\"][i])\n","  val_dataset_dict[\"label\"].append(validation_df[\"label\"][i])\n","  val_dataset_dict[\"span\"].append(validation_df[\"span\"][i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAmHQaKOHken"},"outputs":[],"source":["dataset_dict[\"text\"] = dataset_dict[\"text\"] + val_dataset_dict[\"text\"]\n","dataset_dict[\"label\"] = dataset_dict[\"label\"] + val_dataset_dict[\"label\"]\n","dataset_dict[\"span\"] = dataset_dict[\"span\"] + val_dataset_dict[\"span\"]\n","\n","dataset = Dataset.from_dict(dataset_dict)\n","\n","print(dataset.features)"]},{"cell_type":"markdown","metadata":{"id":"PBOhbeTsPehh"},"source":["#### Adding WEBRADR data to training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2uyU18OUkTv"},"outputs":[],"source":["ref_id_set = set(Counter(reference_dataset_dict[\"label\"]).keys())\n","\n","for id in ref_id_set:\n","  if id in med_id_dict.keys():\n","    reference_dataset_dict[\"label\"] = [med_id_dict[id] if i == id else i for i in reference_dataset_dict[\"label\"]]\n","  else:\n","    reference_dataset_dict[\"label\"] = [map_id if i == id else i for i in reference_dataset_dict[\"label\"]]\n","    med_id_dict[id] = map_id\n","    map_id = map_id + 1\n","\n","print(\"WEBRADR reference dataset has\", len(ref_id_set), \"unique MEDDRA IDs.\")\n","print(\"All sets have\", (map_id), \"unique MEDDRA IDs.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WNw2RcaPjaM"},"outputs":[],"source":["num_labels = map_id\n","\n","dataset_dict[\"text\"] = dataset_dict[\"text\"] + reference_dataset_dict[\"text\"]\n","dataset_dict[\"label\"] = dataset_dict[\"label\"] + reference_dataset_dict[\"label\"]\n","dataset_dict[\"span\"] = dataset_dict[\"span\"] + reference_dataset_dict[\"span\"]\n","\n","dataset = Dataset.from_dict(dataset_dict)\n","\n","print(dataset.features)"]},{"cell_type":"markdown","metadata":{"id":"lbQMUUC7LD76"},"source":["### Validation with the challenge validation Dataset (Trainer)\n","\n","- Define transformer model to be used in classification\n","- Encode the dataset with the embeddings related to the used model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TERr-56d7XPY"},"outputs":[],"source":["# If using bertweet-base\n","!pip3 install emoji==0.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wu_f1bzZLD7_"},"outputs":[],"source":["output_log_file = \"log.csv\"\n","\n","#model_checkpoint = \"bert-base-uncased\"\n","#model_checkpoint = \"bert-large-uncased\"\n","model_checkpoint = \"roberta-base\"\n","#model_checkpoint = \"roberta-large\"\n","#model_checkpoint = \"vinai/bertweet-base\"\n","#model_checkpoint = \"vinai/bertweet-large\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","def preprocess_data_span(examples):\n","  return tokenizer(examples[\"span\"], truncation=True)\n","\n","def preprocess_data(examples):\n","    return tokenizer(examples[\"text\"], truncation=True)\n","\n","#encoded_dataset = dataset.map(preprocess_data, batched = True)\n","#encoded_val_dataset = validation_dataset.map(preprocess_data, batched = True)\n","\n","encoded_dataset = dataset.map(preprocess_data_span, batched = True)\n","encoded_val_dataset = validation_dataset.map(preprocess_data_span, batched = True)\n","\n","pre_tokenizer_columns = set(dataset.features)\n","tokenizer_columns = list(set(encoded_dataset.features) - pre_tokenizer_columns)\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"gO9otjSKLD8D"},"source":["- Define model training parameters\n","- Train the model\n","- Write a log with the model, some parameters and the calculated metrics\n","- Save the trained models on Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nqJJ-d-LD8J"},"outputs":[],"source":["num_epochs = 3\n","batch_size = 32\n","init_lr = 2e-5\n","num_warmup_steps = 0\n","fold = 0\n","\n","#num_labels = len(Counter(df['label']))\n","print(\"Number of labels:\", num_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8klLmSeLD8M"},"outputs":[],"source":["print()\n","print(\"Training Model\")\n","print()\n","\n","encoded_training_dataset = encoded_dataset\n","encoded_validation_dataset = encoded_val_dataset\n","\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/results\",\n","    learning_rate=init_lr,\n","    do_train=True,\n","    do_eval=True,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    weight_decay=0.01,\n","    warmup_steps=num_warmup_steps,\n","    logging_dir=\"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/logs\",\n",")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=encoded_training_dataset,\n","    eval_dataset=encoded_validation_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n","\n","trainer.save_model(\"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/results/checkpoint\")\n"]},{"cell_type":"markdown","metadata":{"id":"H8rjKwbijvR8"},"source":["- Save dictionary with Meddra id key and respective numeric id to load to test the predictions in next step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vY7H0ozAisbb"},"outputs":[],"source":["with open(\"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/results/med_label_dict.tsv\", 'wt') as out_file:\n","  tsv_writer = csv.writer(out_file, delimiter='\\t')\n","  for k in med_id_dict.keys():\n","    tsv_writer.writerow([k, med_id_dict[k]])"]},{"cell_type":"markdown","metadata":{"id":"OZeLeWnSMD_J"},"source":["### Loading model from Google drive and predict validation data\n","\n","- Load model from Drive\n","- Load MedDRA label mapping from Drive\n","- Insert model into text classification pipeline\n","- Output the values of the predictions against the validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imeUF_oJjumX"},"outputs":[],"source":["med_id_dict_file = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/results/med_label_dict.tsv\"\n","\n","med_id_dict_df = pd.read_csv(med_id_dict_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n","\n","med_id_dict = {}\n","inv_med_id_dict = {}\n","\n","for i in range(0, len(med_id_dict_df)):\n","  med_id_dict[med_id_dict_df[0][i]] = int(med_id_dict_df[1][i])\n","  inv_med_id_dict[med_id_dict_df[1][i]] = int(med_id_dict_df[0][i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUmjhmznJkxm"},"outputs":[],"source":["model_dir = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/results/checkpoint\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n","#model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Er7qfzDC3JNY"},"outputs":[],"source":["from transformers import TextClassificationPipeline\n","\n","pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=1)\n","#pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe-WTo44_3gW"},"outputs":[],"source":["l = []\n","\n","for i in range(len(validation_df)):\n","  if (i!=0 and i%100==0):\n","    print(i)\n","  # Use when model was trained with encoded tweets\n","  #l.append(pipe(validation_df[\"text\"][i]))\n","  # Use when model was trained with encoded spans\n","  l.append(pipe(validation_df[\"span\"][i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2TyKqx5CqJQ"},"outputs":[],"source":["l = [i[0][0] for i in l]\n","int(l[1]['label'][6:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOK__fsP3zrl"},"outputs":[],"source":["#validation_df\n","\n","results_file = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/challenge_valdation_results.tsv\"\n","\n","with open(results_file, 'wt') as out_file:\n","  tsv_writer = csv.writer(out_file, delimiter='\\t')\n","  for i in range(len(validation_df)):\n","    label = \"ADE\"\n","    med_id = int(l[i]['label'][6:])\n","    true_med_id = inv_med_id_dict[med_id]\n","    tsv_writer.writerow([validation_df[\"tweet_id\"][i], label, validation_df[\"start\"][i], validation_df[\"end\"][i], validation_df[\"span\"][i], true_med_id])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cIWROSGui0wV"},"source":["### Loading model from Google drive and predict test data\n","\n","- Read test data\n","- Load model from Drive\n","- Load MedDRA label mapping from Drive\n","- Insert model into text classification pipeline\n","- Output the values of the predictions against the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sr5XaH7Li0Nh"},"outputs":[],"source":["test_set = \"/content/drive/MyDrive/Dissertacao/Subtask_1b/outputs/final_test_results.tsv\"\n","\n","test_df = pd.read_csv(test_set, sep='\\t', quoting=csv.QUOTE_NONE, header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Z1OPJm2Iv5f"},"outputs":[],"source":["med_id_dict_file = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/results/med_label_dict.tsv\"\n","\n","med_id_dict_df = pd.read_csv(med_id_dict_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n","\n","med_id_dict = {}\n","inv_med_id_dict = {}\n","\n","for i in range(0, len(med_id_dict_df)):\n","  med_id_dict[med_id_dict_df[0][i]] = int(med_id_dict_df[1][i])\n","  inv_med_id_dict[med_id_dict_df[1][i]] = int(med_id_dict_df[0][i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrpqtc3xIyWx"},"outputs":[],"source":["model_dir = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/results/checkpoint\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n","#model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tokgkH_Sm_qb"},"outputs":[],"source":["from transformers import TextClassificationPipeline\n","\n","pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=1)\n","#pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sz7tQPjsI6Pa"},"outputs":[],"source":["l = []\n","\n","for i in range(len(test_df)):\n","  if (i!=0 and i%100==0):\n","    print(i)\n","  # Use when model was trained with encoded tweets\n","  #l.append(pipe(validation_df[\"text\"][i]))\n","  # Use when model was trained with encoded spans\n","  l.append(pipe(test_df[4][i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ED_zZ4VLJVcG"},"outputs":[],"source":["l = [i[0][0] for i in l]\n","int(l[8]['label'][6:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-3rOVjmJdid"},"outputs":[],"source":["#test_df\n","results_file = \"/content/drive/MyDrive/Dissertacao/Subtask_1c/outputs/final_test_results.tsv\"\n","\n","with open(results_file, 'wt') as out_file:\n","  tsv_writer = csv.writer(out_file, delimiter='\\t')\n","  for i in range(len(test_df)):\n","    label = \"ADE\"\n","    med_id = int(l[i]['label'][6:])\n","    true_med_id = inv_med_id_dict[med_id]\n","    tsv_writer.writerow([test_df[0][i], label, test_df[2][i], test_df[3][i], test_df[4][i], true_med_id])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7rI-QRxbDfir"},"source":["-------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"myfSLBioDfis"},"source":["### Reset Log file\n","\n","- Reset the log file\n","- Only uncomment and run this cell to reset the log file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSunHv_GDfis"},"outputs":[],"source":["# Reset Log File\n","import csv\n","\n","output_log_file = \"log.csv\"\n","#with open(output_log_file, 'w') as log:\n","#        csv_writer = csv.writer(log)\n","#        csv_writer.writerow(['Model', 'Batch_size', 'Init_lr', 'Warmup_steps', 'Fold', 'Epochs', 'Precision', 'Recall', 'F1-score' ])"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.11 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
