{"cells":[{"cell_type":"markdown","metadata":{"id":"nESijxJq9v3T"},"source":["### Mount Google drive\n","\n","*  Mount Google drive in the directory '/content/drive'\n","*  Drive contains dataset files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17283,"status":"ok","timestamp":1665528641734,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"A0iace3RDj3P","outputId":"553382f3-df4c-4544-fa66-1a4de520a411"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"KjQD_DFrqA8u"},"source":["### Install the TextAttack\n","\n","*  Install `textattack[tensorflow]` in order to use data augmentation capabilities of library\n","*  Upgrade numpy in order to avoid error"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167305,"status":"ok","timestamp":1665528809028,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"zBtxn5Z0OY3v","outputId":"ef8b6793-faa8-47bc-9bf5-314911cf0206"},"outputs":[],"source":["# Installing text Attack for the purpose of text augmentation\n","!pip3 install textattack[tensorflow]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9755,"status":"ok","timestamp":1665528818748,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"UuYSwZq_OhQ2","outputId":"00701cfe-dd2a-4af6-dae3-6df6d162f844"},"outputs":[],"source":["# Updating numpy version to avoid exception in text Attack library\n","!pip install --upgrade numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7988,"status":"ok","timestamp":1665528826716,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"SeRLtS445NEn","outputId":"7953a90f-a22c-440f-cea1-80f7929e6f84"},"outputs":[],"source":["!pip install conllu==4.4.1"]},{"cell_type":"markdown","metadata":{"id":"syfgxTDt-SLn"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22817,"status":"ok","timestamp":1665528849500,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"46k4m-gC2d2v","outputId":"0126a42e-6ef9-427b-c2bb-33b38dfd876f"},"outputs":[],"source":["import pandas as pd\n","import csv\n","from collections import Counter\n","import re\n","\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","# import transformations, contraints, and the Augmenter\n","from textattack.transformations import WordSwapRandomCharacterDeletion\n","from textattack.transformations import WordSwapQWERTY\n","from textattack.transformations import CompositeTransformation\n","from textattack.transformations import WordSwapChangeLocation\n","from textattack.transformations import WordSwapChangeName\n","from textattack.transformations import WordSwapChangeNumber\n","from textattack.transformations import WordSwapContract\n","from textattack.transformations import WordSwapWordNet\n","from textattack.transformations import WordSwapRandomCharacterSubstitution\n","\n","from textattack.constraints.pre_transformation import RepeatModification\n","from textattack.constraints.pre_transformation import StopwordModification\n","\n","from textattack.augmentation import Augmenter"]},{"cell_type":"markdown","metadata":{"id":"Xuli_EdFD4pr"},"source":["### Pre-processing\n","\n","* Read training dataset files\n","* Join datasets in a single DataFrame\n","* Lowercase text (optional)\n","* Replacing \"\\&amp;\" for \"&\"\n","* Augment dataset\n","* Save augmented dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1296,"status":"ok","timestamp":1665528850773,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"SpfJEZ1uC_h4"},"outputs":[],"source":["class_file = \"/content/drive/MyDrive/Dissertacao/Subtask_1a/training_data/train_class.tsv\"\n","tweet_file = \"/content/drive/MyDrive/Dissertacao/Subtask_1a/training_data/train_tweets.tsv\"\n","\n","lowercase = False\n","\n","# Read files\n","class_df = pd.read_csv(class_file, sep='\\t', header=None)\n","tweet_df = pd.read_csv(tweet_file, sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n","\n","d = {\"tweet_id\":class_df[0], \"label\":class_df[1], \"text\":tweet_df[1]}\n","\n","df = pd.DataFrame(data = d)\n","\n","for i in range(0, len(df)-1):\n","  if \"&amp;\" in df[\"text\"][i]:\n","    df[\"text\"][i] = df[\"text\"][i].replace(\"&amp;\", \"&\")\n","  if lowercase:\n","    df[\"text\"][i] = df[\"text\"][i].lower()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1665528850774,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"Mbsg8dPzjnrV","outputId":"5f4115d9-6e0a-4e90-a6af-3af53a643908"},"outputs":[],"source":["X = df.text\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1665528850775,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"QXKa3VxRY8V5","outputId":"66dbaa52-1c4d-4b9c-a848-85e29e91cec6"},"outputs":[],"source":["y = df.label\n","y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1665528850776,"user":{"displayName":"Edgar Morais","userId":"09708779788321972011"},"user_tz":-60},"id":"DrgcpRbFdwYK","outputId":"989d140e-b349-4e58-91d2-fa2c81dea89c"},"outputs":[],"source":["print(\"There are \", len(df[df[\"label\"]==\"ADE\"]) , \"positive examples (ADE) in this dataset.\")\n","print(\"There are \", len(df[df[\"label\"]==\"noADE\"]), \"negative examples (NoADE) in this dataset.\")"]},{"cell_type":"markdown","metadata":{"id":"9HOH8HEsFPAs"},"source":["#### Augmentation\n","\n","* Use following transformations:\n","  * Random character swap\n","  * Character swap by adjacent QWERTY keyboard characters\n","  * Perform contractions (For example: \"I am\"->\"I'm\")\n","  * Swap words by Word Net synonyms "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x_FQGcI37Vg"},"outputs":[],"source":["# Set up transformation using CompositeTransformation()\n","#transformation = CompositeTransformation([WordSwapRandomCharacterSubstitution(), WordSwapQWERTY(), WordSwapChangeLocation(), WordSwapChangeName(), WordSwapChangeNumber(), WordSwapContract(), WordSwapWordNet()])\n","# WordSwapRandomCharacterSubstitution - Transforms an input by replacing one character in a word with a random new character.\n","# WordSwapQWERTY - Swaps characters with QWERTY adjacent keys\n","# WordSwapWordNet - Transforms an input by replacing its words with synonyms provided by WordNet\n","# WordSwapContract - Transforms an input by performing contraction on recognized combinations\n","transformation = CompositeTransformation([WordSwapRandomCharacterSubstitution(), WordSwapQWERTY(), WordSwapWordNet(), WordSwapContract()])\n","# Set up constraints\n","constraints = [RepeatModification(), StopwordModification()]\n","# Create augmenter with specified parameters\n","augmenter = Augmenter(transformation=transformation, constraints=constraints, pct_words_to_swap=0.5, transformations_per_example=5)\n","\n","print_var = 1\n","print_count = 0\n","\n","\n","i = 0\n","neg = 0\n","for index, row in df.iterrows():\n","  #print(row[\"label\"])\n","  if(row[\"label\"]==\"ADE\"):\n","    try:\n","      text = re.sub('@\\w+', '@', row[\"text\"])\n","      text = text.replace(\"'\", \"\")\n","      l = augmenter.augment(text)\n","      for n in range(len(l)):\n","        l[n] = l[n].replace(\"@\", \"@USER____\")\n","      new_serie = pd.Series(l)\n","      #if print_var == 1 and print_count < 5:\n","        #print(\"Original tweet:\", text)\n","        #print(\"Generated tweets:\")\n","        #for t in range(5):\n","        #  print(new_serie[t])\n","        #print(\"-----------------------------------\")\n","        #print_count = print_count + 1\n","      X = pd.concat([X, new_serie], ignore_index=True)\n","      ade_serie = pd.Series([\"ADE\"]*5)\n","      y = pd.concat([y, ade_serie], ignore_index=True)\n","      i = i + 1\n","    except IndexError:\n","      #print(\"Index error! Coulg not change tweet -> \", row[\"text\"])\n","      neg = neg + 1\n","    \n","print(i, \"tweets augmented.\")\n","print(\"Could not augment \", neg, \"ADE tweets.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xf_WX8k24UWi"},"outputs":[],"source":["# +-27% positive labels\n","Counter(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2scjhkxMkfJ"},"outputs":[],"source":["# Save augmented dataset\n","with open(\"/content/drive/MyDrive/Dissertacao/Subtask_1a/augmented_training_data/augmented_training.tsv\", 'wt') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['text', 'label'])\n","    for i in range(len(X)):\n","      tsv_writer.writerow([X[i], y[i]])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H6eJeJRFE5rE"},"source":["#### Random Oversampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYmzg6oJFBCS"},"outputs":[],"source":["# Random Oversampler\n","\n","oversampler = RandomOverSampler(sampling_strategy=0.3)\n","\n","X_oversampled, y_oversampled = oversampler.fit_resample(X.to_numpy().reshape(-1,1), y)\n","\n","print(len(X_oversampled))\n","Counter(y_oversampled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIiKAyrjfhCC"},"outputs":[],"source":["# Save oversampled dataset\n","with open(\"/content/drive/MyDrive/Dissertacao/Subtask_1a/oversampled_training_data/oversampled_training.tsv\", 'wt') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['text', 'label'])\n","    for i in range(len(X_oversampled)):\n","      tsv_writer.writerow([X_oversampled[i][0], y_oversampled[i]])"]},{"cell_type":"markdown","metadata":{"id":"jgCsboMNXuio"},"source":["#### Random Undersampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3lPxgDaXykL"},"outputs":[],"source":["# Random Undersampler\n","\n","oversampler = RandomUnderSampler(sampling_strategy=0.1)\n","\n","X_undersampled, y_undersampled = oversampler.fit_resample(X.to_numpy().reshape(-1,1), y)\n","\n","print(len(X_undersampled))\n","Counter(y_undersampled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjdE-rh8YwDw"},"outputs":[],"source":["# Save undersampled dataset\n","with open(\"/content/drive/MyDrive/Dissertacao/Subtask_1a/undersampled_training_data/undersampled_training.tsv\", 'wt') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['text', 'label'])\n","    for i in range(len(X_undersampled)):\n","      tsv_writer.writerow([X_undersampled[i][0], y_undersampled[i]])"]},{"cell_type":"markdown","metadata":{"id":"ABmTHQIIyuzS"},"source":["#### Augmentation and oversampling sequencially\n","\n","* Use following transformations:\n","  * Random character swap\n","  * Character swap by adjacent QWERTY keyboard characters\n","  * Perform contractions (For example: \"I am\"->\"I'm\")\n","  * Swap words by Word Net synonyms\n","* Random oversampling (strategy: 0.4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8OvC_ASyuzZ"},"outputs":[],"source":["# Set up transformation using CompositeTransformation()\n","#transformation = CompositeTransformation([WordSwapRandomCharacterSubstitution(), WordSwapQWERTY(), WordSwapChangeLocation(), WordSwapChangeName(), WordSwapChangeNumber(), WordSwapContract(), WordSwapWordNet()])\n","transformation = CompositeTransformation([WordSwapRandomCharacterSubstitution(), WordSwapQWERTY(), WordSwapWordNet(), WordSwapContract()])\n","# Set up constraints\n","constraints = [RepeatModification(), StopwordModification()]\n","# Create augmenter with specified parameters\n","augmenter = Augmenter(transformation=transformation, constraints=constraints, pct_words_to_swap=0.5, transformations_per_example=5)\n","\n","i = 0\n","neg = 0\n","for index, row in df.iterrows():\n","  #print(row[\"label\"])\n","  if(row[\"label\"]==\"ADE\"):\n","    try:\n","      text = re.sub('@\\w+', '@', row[\"text\"])\n","      text = text.replace(\"'\", \"\")\n","      l = augmenter.augment(text)\n","      for n in range(len(l)):\n","        l[n] = l[n].replace(\"@\", \"@USER____\")\n","      new_serie = pd.Series(l)\n","      X = pd.concat([X, new_serie], ignore_index=True)\n","      ade_serie = pd.Series([\"ADE\"]*5)\n","      y = pd.concat([y, ade_serie], ignore_index=True)\n","      i = i + 1\n","    except IndexError:\n","      #print(\"Index error! Coulg not change tweet -> \", row[\"text\"])\n","      neg = neg + 1\n","    \n","print(i, \"tweets augmented.\")\n","print(\"Could not augment \", neg, \"ADE tweets.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccwpRGshyuzj"},"outputs":[],"source":["# +-27% positive labels\n","Counter(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zpn3sLXPzAaQ"},"outputs":[],"source":["# Random Oversampler\n","\n","oversampler = RandomOverSampler(sampling_strategy=0.4)\n","\n","X_oversampled, y_oversampled = oversampler.fit_resample(X.to_numpy().reshape(-1,1), y)\n","\n","print(len(X_oversampled))\n","Counter(y_oversampled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAY9g_58zAaV"},"outputs":[],"source":["# Save oversampled dataset\n","with open(\"/content/drive/MyDrive/Dissertacao/Subtask_1a/augmented_oversampled_training_data/augmented_oversampled_training.tsv\", 'wt') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['text', 'label'])\n","    for i in range(len(X_oversampled)):\n","      tsv_writer.writerow([X_oversampled[i][0], y_oversampled[i]])"]},{"cell_type":"markdown","metadata":{"id":"CWUl_OoShGqv"},"source":["### Merging dataset to single file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D4Sv5bbhLEw"},"outputs":[],"source":["print(len(X))\n","Counter(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASsWJpb6hpqk"},"outputs":[],"source":["# Save oversampled dataset\n","with open(\"/content/drive/MyDrive/Dissertacao/Subtask_1a/training_data/merged_training_dataset.tsv\", 'wt') as out_file:\n","    tsv_writer = csv.writer(out_file, delimiter='\\t')\n","    tsv_writer.writerow(['text', 'label', 'start', 'end', 'span', 'med_id'])\n","    for i in range(len(X)):\n","      tsv_writer.writerow([X[i], y[i]])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO6+GTLvZaa89JuxCGat1Ig","collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.11 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.11"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
